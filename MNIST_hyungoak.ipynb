{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_hyungoak.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brotheroak/Tensorflow/blob/master/MNIST_hyungoak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivB0ST6OBY5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "f68528f2-da58-48b0-ab10-63b51aca6e44"
      },
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt  # for plotting\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "nb_classes = 10  # classes 0 - 9\n",
        "\n",
        "# The file path to save the data\n",
        "save_file = './model'\n",
        "\n",
        "# MNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "# 0 - 9 digits recognition = 10 classes\n",
        "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "X_image = tf.reshape(X, [-1,28,28,1])\n",
        "\n",
        "W1 = tf.Variable(tf.random_normal([5, 5, 1, 32], stddev=0.01))\n",
        "L1 = tf.nn.relu(tf.nn.conv2d(X_image, filter=W1, strides=[1, 1, 1, 1, ], padding='SAME'))\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([5, 5, 32, 64], stddev=0.01))\n",
        "L2 = tf.nn.relu(tf.nn.conv2d(L1, filter=W2, strides=[1, 1, 1, 1, ], padding='SAME'))\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
        "L2 = tf.nn.dropout(L2, keep_prob)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([7 * 7 * 64, 512], stddev=0.01))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3))\n",
        "L3 = tf.nn.dropout(L3, keep_prob)\n",
        "\n",
        "W4 = tf.Variable(tf.random_normal([512, nb_classes], stddev=0.01))\n",
        "B4 = tf.Variable(tf.random_normal(shape=[nb_classes], stddev=0.01))\n",
        "\n",
        "# simple single layer perceptron (784+1(bias) to 10)\n",
        "# W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
        "# b = tf.Variable(tf.random_normal([nb_classes]))\n",
        "\n",
        "# Class used to save and/or restore Tensor Variables\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# Hypothesis (using softmax)\n",
        "# hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(L3, W4) + B4)\n",
        "\n",
        "# Cross Entropy Cost\n",
        "# cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(tf.clip_by_value(hypothesis, 1e-10, 1.0)), [1]))\n",
        "\n",
        "# Use SGD as Optimizer\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Use ADAM as Optimizer\n",
        "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
        "\n",
        "# Test model (for verifying accuracy)\n",
        "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# Parameters\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "learning_rate = 0.01\n",
        "display_epoch = 1\n",
        "logs_path = '/tmp/tensorflow_logs/example/'\n",
        "\n",
        "# Create a summary to monitor cost tensor\n",
        "tf.summary.scalar(\"loss\", cost)\n",
        "# Create a summary to monitor accuracy tensor\n",
        "tf.summary.scalar(\"accuracy\", accuracy)\n",
        "# Merge all summaries into a single op\n",
        "merged_summary_op = tf.summary.merge_all()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-ebdf0641365a>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From <ipython-input-1-ebdf0641365a>:26: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-1-ebdf0641365a>:63: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.math.argmax` instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QQqLmrPAvNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "2ba173ac-3ceb-4222-8be0-eaae33f89ed1"
      },
      "source": [
        "# Training & Test Session\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # If sample_size=60000 and batch_size=100, then total batch count is 600\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    # op to write logs to Tensorboard\n",
        "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0  # for accumulation\n",
        "        # If sample_size=60000 and batch_size=100, then total batch count is 600\n",
        "        # total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "        # Per each batch\n",
        "        for i in range(total_batch):\n",
        "            # Use built-in function (auto shuffling+sampling)\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            \n",
        "            # Run optimization op (backprop), cost op (to get loss value)\n",
        "            # and summary nodes\n",
        "            _, c, summary = sess.run([optimizer, cost, merged_summary_op],\n",
        "                                     feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})\n",
        "            \n",
        "            # Write logs at every iteration\n",
        "            summary_writer.add_summary(summary, epoch * total_batch + i)\n",
        "\n",
        "            avg_cost += c / total_batch  # avg_cost of this epoch\n",
        "\n",
        "        print('Epoch:', '%04d' % (epoch + 1),\n",
        "              'cost =', '{:.9f}'.format(avg_cost))\n",
        "    print(\"Learning finished\")\n",
        "    # Test the model using test sets\n",
        "    # print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
        "    #       X: mnist.test.images, Y: mnist.test.labels}))\n",
        "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
        "          X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1.0}))\n",
        "    \n",
        "    # Get one and predict\n",
        "    r = random.randint(0, mnist.test.num_examples - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "    # Run testing\n",
        "    # print(\"Prediction: \", sess.run(\n",
        "    #     tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
        "    print(\"Prediction: \", sess.run(\n",
        "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1.0}))\n",
        "\n",
        "    # Selected sample showing\n",
        "    plt.imshow(\n",
        "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
        "        cmap='Greys',\n",
        "        interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "    # Save the model\n",
        "    saver.save(sess, save_file)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.263677528\n",
            "Epoch: 0002 cost = 0.077087355\n",
            "Epoch: 0003 cost = 0.057015955\n",
            "Epoch: 0004 cost = 0.045583029\n",
            "Epoch: 0005 cost = 0.036053425\n",
            "Epoch: 0006 cost = 0.034702368\n",
            "Epoch: 0007 cost = 0.029957265\n",
            "Epoch: 0008 cost = 0.026259423\n",
            "Epoch: 0009 cost = 0.022774927\n",
            "Epoch: 0010 cost = 0.020807166\n",
            "Epoch: 0011 cost = 0.021425828\n",
            "Epoch: 0012 cost = 0.016280214\n",
            "Epoch: 0013 cost = 0.017855034\n",
            "Epoch: 0014 cost = 0.017417005\n",
            "Epoch: 0015 cost = 0.016203488\n",
            "Learning finished\n",
            "Accuracy:  0.9932\n",
            "Label:  [8]\n",
            "Prediction:  [8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfkt02sSB6wK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(logs_path)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA3_0NeUDBAt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "074ce07e-188b-4612-9834-b5ffbdf83db9"
      },
      "source": [
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-06 13:38:01--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 3.219.201.17, 52.200.233.201, 52.200.123.104, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|3.219.201.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13607069 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  12.98M  6.49MB/s    in 2.0s    \n",
            "\n",
            "2019-10-06 13:38:04 (6.49 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13607069/13607069]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk4HDVBKDCfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIr2lFIYDENx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cbb1820-6331-46c2-f247-ba4bb384744e"
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://3477689b.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdgJDxNRSq8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}